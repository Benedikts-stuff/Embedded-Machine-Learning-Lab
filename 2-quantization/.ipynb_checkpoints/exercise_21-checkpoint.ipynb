{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "detected-pipeline",
   "metadata": {},
   "source": [
    "## Embedded ML Lab - Excercise 2 - Quantization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dated-texture",
   "metadata": {},
   "source": [
    "The goal of this exercise is to take a given network, fuse its operators, and finally quantize it. For that we will do the following steps\n",
    "* 1) We define the quantized network with fused operators\n",
    "* 2) We determine how to fuse `conv-bn-relu` structures into a single quantized operation.\n",
    "* 3) We fuse the weights from the pre-trained state dict and quantize them\n",
    "* 4) We use a calibration batch from the pretrained network to determine all required scales\n",
    "* 5) Done :)\n",
    "\n",
    "For this lab the non-quantized version of the net we use is already implemented in `net.py`. It contains 6 conv, 6 batchnorm, 6 relu layers, and only has a very small linear part at the end. Take a look at it.\n",
    "\n",
    "<img src=\"src/cifarnet.png\" alt=\"drawing\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "integrated-victor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import CifarNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dress-dollar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.backends.quantized.engine = 'qnnpack'\n",
    "\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "tf = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "testloader = torch.utils.data.DataLoader(torchvision.datasets.CIFAR10('data/', train=False, download=True, transform=tf), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-mailman",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "To measure the effects of quantization we want to measure the time it takes to calculate a batch with the quantized and the unquatized network to run on the cpu. Additionally, we want to know what the accuracy penalty is.\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Implement a function `net_time` that measures the time it takes (forward pass) to process a batch with size 32 of cifar100. You can use `t_now = time.time()` to get the current time.</span>\n",
    "    * <span style=\"color:green\">NOTE: To save time, you do not have to iterate over the whole dataset.</span>\n",
    "* <span style=\"color:green\">Implement a function `net_acc` that measures the accuracy of the net class, and takes the class type, a state_dict, and a dataloader as input.</span>\n",
    "    * <span style=\"color:green\">NOTE: To save time, you do not have to iterate over the whole dataset.</span>\n",
    "    * <span style=\"color:green\">NOTE: You can reuse code from the last lab exercises.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "included-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def correct_predictions(outputs, targets):\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(targets)):\n",
    "        correct_predictions += int(torch.argmax(outputs[i]) == targets[i])\n",
    "    return correct_predictions\n",
    "\n",
    "def net_time(model_class, testloader):\n",
    "    \n",
    "    #----to-be-done-by-student-------------------\n",
    "    net = model_class()\n",
    "    batch, labels = next(iter(testloader))\n",
    "    t_now = time.time()\n",
    "    forward = net(batch)\n",
    "    t_after = time.time()\n",
    "    #----to-be-done-by-student-------------------\n",
    "    t = abs(t_now - t_after)\n",
    "    return t\n",
    "\n",
    "def net_acc(model_class, state_dict, testloader):\n",
    "    #----to-be-done-by-student-------------------\n",
    "    net = model_class()\n",
    "    net.load_state_dict(state_dict)\n",
    "    batch, labels = next(iter(testloader))\n",
    "    forward = net(batch)\n",
    "    num_correct = correct_predictions(forward, labels)\n",
    "    num_samples = len(labels)\n",
    "    #----to-be-done-by-student-------------------\n",
    "    accuracy = num_correct / num_samples\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "periodic-preliminary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time unquantized: 0.1515662670135498 s\n",
      "Accuracy unquantized: 84.3750%\n"
     ]
    }
   ],
   "source": [
    "print(f'Time unquantized: {net_time(CifarNet, testloader)} s')\n",
    "print(f\"Accuracy unquantized: {net_acc(CifarNet, torch.load('state_dict.pt'), testloader):.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-priority",
   "metadata": {},
   "source": [
    "## Quantized network\n",
    "Now we define the quantized version of CifarNet with fused operators ( conv-bn-relu -> qfused_conv_relu). The resulting network has a structure as shown below:\n",
    "\n",
    "<img src=\"src/cifarnet_quantized.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Take the provided image as well as the CifarNet implementation as reference and implemenet the **forward pass** of QCifarNet.</span>\n",
    "    * <span style=\"color:green\">The required modules `Conv2drelu` and `QLinear` are already provided and can be used like any other module we have seen before. Note that these modules require their weights to be quantized, the bias is unquantized. The forward pass of these modules require an quantized input and return an quantized output. The modules are essentially only a wrapper with parameters around `torch.ops.quantized.conv2d_relu` and `torch.ops.quantized.linear`. Additionally these modules have an paramter called `scale`, that is used as ouput scale for the operation.</span>\n",
    "    * <span style=\"color:green\">You might require some other \"stateless\" operators such as `torch.quantize_per_tensor`, `torch.dequantize`,`torch.flatten`, and `torch.nn.quantized.functional.max_pool2d`.</span>\n",
    "* <span style=\"color:green\">Profile the resulting net and compare its forward pass time to the non-quantized implementation.</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "sudden-fifteen",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def f_sd(sd, endswith_key_string):\n",
    "    keys = [i for i in sd.keys() if i.endswith(endswith_key_string)]\n",
    "    if not keys:\n",
    "        raise KeyError(endswith_key_string)\n",
    "    return sd[keys[0]]\n",
    "\n",
    "#Quantized Conv2dReLU Module\n",
    "class QConv2dReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(QConv2dReLU, self).__init__()\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.quantize_per_tensor(torch.Tensor(\n",
    "                out_channels, in_channels // 1, *(kernel_size, kernel_size)), scale=0.1, zero_point = 0, dtype=torch.qint8), requires_grad=False)\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_channels), requires_grad=False)\n",
    "\n",
    "        self.register_buffer('scale', torch.tensor(0.1))\n",
    "\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "        self._prepack = self._prepare_prepack(self.weight, self.bias, stride, padding)\n",
    "        self._register_load_state_dict_pre_hook(self._sd_hook)\n",
    "\n",
    "    def _prepare_prepack(self, qweight, bias, stride, padding):\n",
    "        assert qweight.is_quantized, \"QConv2dReLU requires a quantized weight.\"\n",
    "        assert not bias.is_quantized, \"QConv2dReLU requires a float bias.\"\n",
    "        return torch.ops.quantized.conv2d_prepack(qweight, bias, stride=[stride, stride], dilation=[1,1], padding=[padding, padding], groups=1)\n",
    "\n",
    "    \n",
    "    def _sd_hook(self, state_dict, prefix, *_):\n",
    "        self._prepack = self._prepare_prepack(f_sd(state_dict, prefix + 'weight'), f_sd(state_dict, prefix + 'bias'),\n",
    "                                             self.stride, self.padding)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.ops.quantized.conv2d_relu(x, self._prepack, self.scale, 64)\n",
    "\n",
    "    \n",
    "#Quantized Linear Module\n",
    "class QLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(QLinear, self).__init__()\n",
    "\n",
    "        self.weight = torch.nn.Parameter(torch.quantize_per_tensor(torch.Tensor(out_features, in_features), scale=0.1, zero_point = 0, dtype=torch.qint8), requires_grad=False)\n",
    "        self.bias = torch.nn.Parameter(torch.Tensor(out_features))\n",
    "\n",
    "        self.register_buffer('scale', torch.tensor(0.1))\n",
    "        \n",
    "        self._prepack = self._prepare_prepack(self.weight, self.bias)\n",
    "        \n",
    "        self._register_load_state_dict_pre_hook(self._sd_hook)\n",
    "        \n",
    "    def _prepare_prepack(self, qweight, bias):\n",
    "        assert qweight.is_quantized, \"QConv2dReLU requires a quantized weight.\"\n",
    "        assert not bias.is_quantized, \"QConv2dReLU requires a float bias.\"\n",
    "        return torch.ops.quantized.linear_prepack(qweight, bias)\n",
    "    \n",
    "    def _sd_hook(self, state_dict, prefix, *_):\n",
    "        self._prepack = self._prepare_prepack(f_sd(state_dict, prefix + 'weight'), f_sd(state_dict, prefix + 'bias'))\n",
    "        return\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.ops.quantized.linear(x, self._prepack, self.scale, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "possible-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_dict of QConv2dReLU\n",
      "weight torch.qint8\n",
      "bias torch.float32\n",
      "scale torch.float32\n",
      "\n",
      "state_dict of QLinear\n",
      "weight torch.qint8\n",
      "bias torch.float32\n",
      "scale torch.float32\n"
     ]
    }
   ],
   "source": [
    "print('state_dict of QConv2dReLU')\n",
    "qconv2drelu = QConv2dReLU(3, 16)\n",
    "for key in qconv2drelu.state_dict(): print(key, qconv2drelu.state_dict()[key].dtype)\n",
    "print('\\nstate_dict of QLinear')\n",
    "qlinear = QLinear(10, 10)\n",
    "for key in qlinear.state_dict(): print(key, qlinear.state_dict()[key].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "incorrect-republic",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QCifarNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QCifarNet, self).__init__()\n",
    "        \n",
    "        self.register_buffer(\"scale\", torch.tensor(0.1))\n",
    "\n",
    "        self.conv1 = QConv2dReLU(3, 16, 3, 1, padding=1)\n",
    "        self.conv2 = QConv2dReLU(16,16, 3, 1, padding=1)\n",
    "\n",
    "        self.conv3 = QConv2dReLU(16, 32, 3, 1, padding=1)\n",
    "        self.conv4 = QConv2dReLU(32, 32, 3, 1, padding=1)\n",
    "\n",
    "        self.conv5 = QConv2dReLU(32, 64, 3, 1, padding=1)\n",
    "        self.conv6 = QConv2dReLU(64, 64, 3, 1, padding=1)\n",
    "\n",
    "        self.fc = QLinear(1024, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #to-be-done-by-student\n",
    "        x = self.conv1(torch.quantize_per_tensor(x, self.scale, 0, torch.quint8))\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = nn.quantized.functional.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = nn.quantized.functional.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = nn.quantized.functional.max_pool2d(x, 2)\n",
    "        \n",
    "        x = x.flatten(1, -1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.dequantize(x)\n",
    "        #to-be-done-by-student\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "unexpected-brass",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time quantized: 0.10202264785766602 s\n"
     ]
    }
   ],
   "source": [
    "#We evaulate how fast the quantized verions of CifarNet is\n",
    "print(f\"Time quantized: {net_time(QCifarNet, testloader)} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-occasion",
   "metadata": {},
   "source": [
    "## Calibration and Operator Fusion\n",
    "\n",
    "First we focus on the operator fusion:\n",
    "* We need calculate the new weights (fused conv and batchnorm weights). After we have weights, we can quantize them using the `tensor_scale` equation from earlier.\n",
    "    * A Conv2d convolution can be expressed as $y_i = \\boldsymbol{ W_{i}} \\star x + b_{_i}$, where $y_i$ is the channel wise output of the convolution and $\\boldsymbol{ W_{i}}$ is a $\\text{channel_in} \\times \\text{kernel_size} \\times \\text{kernel_size}$ kernel.\n",
    "    * The batch_norm operation looks like this: $\\hat x_i = \\frac{x_i - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$, where for each output channel of a convolution $i \\in C$, we scale and shift the input to be zero mean and unit variance, where $\\mu_i$ is the channel wise input mean, and $\\sigma^2_i$ is the channels wise variance. Parameter $\\epsilon$ is added for numerical stability.\n",
    "    * After this shift and scale operation trainable weight and bias terms are added\n",
    " $y_i = \\gamma_i \\hat x_i + \\beta_i$, where $\\gamma_i$ is a channel wise scale factor and $\\beta_i$ is a channel wise bias.\n",
    "    * We can express the batchnorm operation as $y_i = (\\frac{\\gamma_i} {\\sqrt{\\sigma_i^2 + \\epsilon}})x_i +  (\\frac{ - \\mu_i \\gamma_i}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta_i)$ and fuse it with the convolution kernel by using $y_i = (\\frac{\\gamma_i} {\\sqrt{\\sigma_i^2 + \\epsilon}} \\boldsymbol{ W_i}) \\star x_i +  (\\frac{ \\gamma_i ( b_i - \\mu_i)}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta_i)$, s.t. the fused kernel (output channel wise) can be expressed as $\\tilde{\\boldsymbol{ W_{i}}} = (\\frac{\\gamma_i} {\\sqrt{\\sigma_i^2 + \\epsilon}}) \\boldsymbol{ W_i}$ and the fused bias (output channel wise) as $\\tilde{b_i} = (\\frac{ \\gamma_i ( b_i - \\mu_i)}{\\sqrt{\\sigma_i^2 + \\epsilon}} + \\beta_i)$ .\n",
    " \n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>\n",
    "* <span style=\"color:green\">Implement a function `fuse_conv_bn_weights` that fuses the weights and bias of the convolution with the weights, bias, running_mean and running_var of the batchnorm_layer</span>\n",
    "    * <span style=\"color:green\"> determine $\\tilde{b}$ and $\\tilde{\\boldsymbol{ W}}$</span>\n",
    "    * <span style=\"color:green\"> You can either do this channel by channel or compleatly vectorized</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "linear-connecticut",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_scale(input):\n",
    "    return float(2*torch.max(torch.abs(torch.max(input)), torch.abs(torch.min(input))))/127.0\n",
    "\n",
    "def fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_w, bn_b):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        conv_w: shape=(output_channels, in_channels, kernel_size, kernel_size)\n",
    "        conv_b: shape=(output_channels)\n",
    "        bn_rm:  shape=(output_channels)\n",
    "        bn_rv:  shape=(output_channels)\n",
    "        bn_w:   shape=(output_channels)\n",
    "        bn_b:   shape=(output_channels)\n",
    "    \n",
    "    Output:\n",
    "        fused_conv_w = shape=conv_w\n",
    "        fused_conv_b = shape=conv_b\n",
    "    \"\"\"\n",
    "    bn_eps = 1e-05\n",
    "\n",
    "    fused_conv = torch.zeros(conv_w.shape)\n",
    "    fused_bias = torch.zeros(conv_b.shape)\n",
    "\n",
    "    #to-be-done-by-student\n",
    "    fused_conv = (bn_w / torch.sqrt(bn_rv + bn_eps)) * conv_w\n",
    "    fused_bias = (bn_w * (conv_b - bn_rm) / torch.sqrt(bn_rv + bn_eps) + bn_b)\n",
    "    #to-be-done-by-student\n",
    "\n",
    "    return fused_conv, fused_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-clark",
   "metadata": {},
   "source": [
    "Now that we know how to fuse conv and batchnorm layers, we can setup the quantized state dict. We have to take the unfused unquantized parameters of the unquantized pretrained network (`state_dict.pt`) and fuse and quantize them.\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>  \n",
    "* <span style=\"color:green\">Now for each Conv weights and biases, load the pre-trained float weights and biases from the saved state_dict, fuse the corresponding weights and biases with the batch norm weights, biases, mean, and variance, and store the fused quantized weight into the quantized state_dict `qsd`</span>\n",
    "* <span style=\"color:green\">Some Tips:</span>\n",
    "    * <span style=\"color:green\">Print out the keys from the unquantized and quantized state_dict and see what is inside.</span>\n",
    "    * <span style=\"color:green\">You can ignore the scales for now, we will take care of them later.</span>\n",
    "    * <span style=\"color:green\">Reuse the function `tensor_scale`</span>\n",
    "    * <span style=\"color:green\">Weights require to be of type torch.qint8, therefor have a zero_point of 0.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "established-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.float32\n",
      "conv1.bias torch.float32\n",
      "conv2.weight torch.float32\n",
      "conv2.bias torch.float32\n",
      "conv3.weight torch.float32\n",
      "conv3.bias torch.float32\n",
      "conv4.weight torch.float32\n",
      "conv4.bias torch.float32\n",
      "conv5.weight torch.float32\n",
      "conv5.bias torch.float32\n",
      "conv6.weight torch.float32\n",
      "conv6.bias torch.float32\n",
      "bn1.weight torch.float32\n",
      "bn1.bias torch.float32\n",
      "bn1.running_mean torch.float32\n",
      "bn1.running_var torch.float32\n",
      "bn1.num_batches_tracked torch.int64\n",
      "bn2.weight torch.float32\n",
      "bn2.bias torch.float32\n",
      "bn2.running_mean torch.float32\n",
      "bn2.running_var torch.float32\n",
      "bn2.num_batches_tracked torch.int64\n",
      "bn3.weight torch.float32\n",
      "bn3.bias torch.float32\n",
      "bn3.running_mean torch.float32\n",
      "bn3.running_var torch.float32\n",
      "bn3.num_batches_tracked torch.int64\n",
      "bn4.weight torch.float32\n",
      "bn4.bias torch.float32\n",
      "bn4.running_mean torch.float32\n",
      "bn4.running_var torch.float32\n",
      "bn4.num_batches_tracked torch.int64\n",
      "bn5.weight torch.float32\n",
      "bn5.bias torch.float32\n",
      "bn5.running_mean torch.float32\n",
      "bn5.running_var torch.float32\n",
      "bn5.num_batches_tracked torch.int64\n",
      "bn6.weight torch.float32\n",
      "bn6.bias torch.float32\n",
      "bn6.running_mean torch.float32\n",
      "bn6.running_var torch.float32\n",
      "bn6.num_batches_tracked torch.int64\n",
      "fc.weight torch.float32\n",
      "fc.bias torch.float32\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1024) must match the size of tensor b (16) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m sd: \u001b[38;5;28mprint\u001b[39m(key, sd[key]\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     10\u001b[0m W_1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantize_per_tensor(sd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn1.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m], tensor_scale(sd[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbn1.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m]), \u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mquint8)\n\u001b[0;32m---> 11\u001b[0m W_1_dach \u001b[38;5;241m=\u001b[39m \u001b[43mfuse_conv_bn_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43msd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv1.weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconv1.bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbn1.bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfc.weight\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msd\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfc.bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#-to-be-done- by student \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[71], line 24\u001b[0m, in \u001b[0;36mfuse_conv_bn_weights\u001b[0;34m(conv_w, conv_b, bn_rm, bn_rv, bn_w, bn_b)\u001b[0m\n\u001b[1;32m     21\u001b[0m fused_bias \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(conv_b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#to-be-done-by-student\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m fused_conv \u001b[38;5;241m=\u001b[39m (\u001b[43mbn_w\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbn_rv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbn_eps\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m conv_w\n\u001b[1;32m     25\u001b[0m fused_bias \u001b[38;5;241m=\u001b[39m (bn_w \u001b[38;5;241m*\u001b[39m (conv_b \u001b[38;5;241m-\u001b[39m bn_rm) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(bn_rv \u001b[38;5;241m+\u001b[39m bn_eps) \u001b[38;5;241m+\u001b[39m bn_b)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#to-be-done-by-student\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (1024) must match the size of tensor b (16) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "#prints keys from quantized net\n",
    "qnet = QCifarNet()\n",
    "qsd = qnet.state_dict()\n",
    "#for key in qsd: print(key, qsd[key].dtype)\n",
    "\n",
    "sd = torch.load('state_dict.pt')\n",
    "\n",
    "#-to-be-done- by student\n",
    "for key in sd: print(key, sd[key].dtype)\n",
    "conv_w = torch.quantize_per_tensor(sd[\"conv1.weight\"], tensor_scale(sd[\"conv1.weight\"]), 0, torch.quint8)\n",
    "conv_b = torch.quantize_per_tensor(sd[\"bn1.weight\"], tensor_scale(sd[\"conv1.weight\"]), 0, torch.quint8)\n",
    "bn_rm = sd[\"bn1.running_mean\"]\n",
    "bn_rv = sd[\"bn1.running_var\"]\n",
    "bn_w = sd[\"bn1.weight\"]\n",
    "bn_b = sd[\"bn1.bias\"]\n",
    "W_1_dach = fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_w, bn_b)\n",
    "#-to-be-done- by student "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-triangle",
   "metadata": {},
   "source": [
    "Now that we have the fused parameters, we still require the right scales for the activations. For that we \"observe\" the activation scales in the unquantized network using a calibration \"batch\", reuse the function `tensor_scale`\n",
    "\n",
    "<span style=\"color:green\">Your Tasks:</span>  \n",
    "* <span style=\"color:green\">Directly calculate the required scales in the forward pass, e.g. the scale for the inital quantization, and the output scale for each fused operation, and final output scale (the output of the FC layer).</span>\n",
    "* <span style=\"color:green\">There is already an inherited version of CifarNet provided, where you only have to redefine the forward pass and add the calculated scales to the `calibration_dict`. We will later use them to set the remaining scales in our quantized state_dict.</span>\n",
    "* <span style=\"color:green\">It is sufficient to estimate the scales in only one forward pass (we can make the batchsize large).</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarNetCalibration(CifarNet):\n",
    "    def __init__(self):\n",
    "        super(CifarNetCalibration, self).__init__()\n",
    "        \n",
    "        #we add a new dict for the corresponding scales\n",
    "        self.calibration_dict = {}\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #to-be-done-by-student\n",
    "        ###\n",
    "        ###\n",
    "        ###\n",
    "        #--to---be---done---by---student\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-myanmar",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the calibration using a batch from the testdata\n",
    "net_calib = CifarNetCalibration()\n",
    "net_calib.load_state_dict(torch.load('state_dict.pt'))\n",
    "_, (data, _) = next(enumerate(testloader))\n",
    "net_calib(data)\n",
    "calibration_dict = net_calib.calibration_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressive-sentence",
   "metadata": {},
   "source": [
    "<span style=\"color:green\">Your Task:</span>  \n",
    "* <span style=\"color:green\">Now, transfer the scales into the state_dict `qsd`.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-to-be-done- by student \n",
    "###\n",
    "###\n",
    "#-to-be-done- by student "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We run the accuracy test again to see how much accuracy we loose through quantization\n",
    "print(f'Time quantized: {net_time(QCifarNet, testloader)} s')\n",
    "print(f\"Accuracy quantized: {net_acc(QCifarNet, qsd, testloader):.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-greece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
